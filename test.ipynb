{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OpenAI.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m api_key = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# OpenAI 클라이언트 초기화\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# API 키는 환경 변수에서 자동 로드됨.\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 또는 client = OpenAI(api_key=\"YOUR_API_KEY\") 로 직접 설정\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 1. PDF 텍스트 추출 (페이지 번호 포함)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_text_with_page_info\u001b[39m(pdf_path):\n",
      "\u001b[31mTypeError\u001b[39m: OpenAI.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import json # LLM 응답을 JSON으로 파싱하기 위함\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# OpenAI 클라이언트 초기화\n",
    "# API 키는 환경 변수에서 자동 로드됨.\n",
    "# 또는 client = OpenAI(api_key=\"YOUR_API_KEY\") 로 직접 설정\n",
    "client = OpenAI(api_key)\n",
    "\n",
    "# 1. PDF 텍스트 추출 (페이지 번호 포함)\n",
    "def extract_text_with_page_info(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 페이지별로 추출하고, 각 페이지 시작에 페이지 번호를 표기합니다.\n",
    "    \"\"\"\n",
    "    document = fitz.open(pdf_path)\n",
    "    full_text_with_pages = []\n",
    "    for page_num in range(document.page_count):\n",
    "        page = document.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        # 각 페이지 시작에 페이지 번호를 명시적으로 추가\n",
    "        full_text_with_pages.append(f\"\\n---PAGE_START_{page_num+1}---\\n{text}\")\n",
    "    return \"\\n\".join(full_text_with_pages)\n",
    "\n",
    "# 2. 초기 텍스트 분할 (LLM 컨텍스트에 맞게)\n",
    "def initial_text_split(text, chunk_size=4000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    텍스트를 LLM이 처리할 수 있는 크기로 초기 분할합니다.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        # 분할 우선순위: 큰 의미 단위 -> 작은 의미 단위\n",
    "        separators=[\"\\n---PAGE_START_\\d+---\\n\", \"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"]\n",
    "    )\n",
    "    docs = text_splitter.create_documents([text])\n",
    "    return docs\n",
    "\n",
    "# 3. LLM을 이용한 의미적 청킹 (요구사항 추출 예시)\n",
    "def llm_based_semantic_chunking(initial_chunks, llm_model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    각 초기 청크를 LLM에 전달하여 의미 있는 작은 청크 (예: 개별 요구사항)를 추출합니다.\n",
    "    \"\"\"\n",
    "    final_semantic_chunks = []\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    당신은 RFP(제안요청서) 문서에서 핵심 요구사항을 정확하게 추출하고 구조화하는 전문 AI입니다.\n",
    "    사용자가 제공하는 텍스트는 RFP 문서의 일부입니다.\n",
    "    텍스트에서 모든 기능적(Functional) 및 비기능적(Non-Functional) 요구사항을 식별하여 추출해야 합니다.\n",
    "    각 요구사항은 하나의 독립적인 의미 단위를 구성해야 하며, 간결하고 명확하게 요약되어야 합니다.\n",
    "    각 요구사항의 출처 페이지를 반드시 명시하세요. (예: [페이지 N])\n",
    "    \n",
    "    응답은 다음 JSON 배열 형식으로만 제공해야 합니다.\n",
    "    [\n",
    "        {{\n",
    "            \"id\": \"REQ-001\",\n",
    "            \"type\": \"기능적\" or \"비기능적\" or \"제약사항\" or \"기타\",\n",
    "            \"description\": \"요구사항의 간결한 설명\",\n",
    "            \"source_pages\": [페이지 번호1, 페이지 번호2],\n",
    "            \"raw_text_snippet\": \"해당 요구사항이 포함된 원본 텍스트의 일부\"\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "    만약 요구사항이 없다면 빈 배열 `[]`을 반환하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    for i, chunk_doc in enumerate(initial_chunks):\n",
    "        chunk_text = chunk_doc.page_content\n",
    "        print(f\"--- LLM 처리 중: 청크 {i+1}/{len(initial_chunks)} (길이: {len(chunk_text)}자) ---\")\n",
    "\n",
    "        # 초기 청크에서 페이지 번호 범위 추출\n",
    "        # 첫 번째 페이지 시작 표시에서 페이지 번호 추출 (fallback)\n",
    "        match = re.search(r'---PAGE_START_(\\d+)---', chunk_text)\n",
    "        start_page_in_chunk = int(match.group(1)) if match else 1\n",
    "\n",
    "        # 청크 내의 모든 페이지 시작 표시를 찾아 페이지 범위 지정\n",
    "        page_numbers_in_chunk = sorted(list(set(\n",
    "            int(p) for p in re.findall(r'---PAGE_START_(\\d+)---', chunk_text)\n",
    "        )))\n",
    "        \n",
    "        # 청크가 여러 페이지에 걸쳐있을 경우, 범위로 표현\n",
    "        if page_numbers_in_chunk:\n",
    "            # 청크 내 페이지 번호들을 실제 페이지 번호로 사용\n",
    "            current_pages_str = f\"페이지 범위: {page_numbers_in_chunk[0]} - {page_numbers_in_chunk[-1]}\"\n",
    "        else:\n",
    "            current_pages_str = f\"추정 페이지: {start_page_in_chunk}\"\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "        다음은 RFP 문서의 일부입니다. 이 부분에서 모든 요구사항을 JSON 형식으로 추출해 주세요.\n",
    "        현재 처리 중인 텍스트의 추정 페이지 범위는 {current_pages_str} 입니다.\n",
    "\n",
    "        --- 텍스트 ---\n",
    "        {chunk_text}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                response_format={\"type\": \"json_object\"}, # JSON 응답을 강제\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.0 # 창의성 최소화, 정확한 추출 지향\n",
    "            )\n",
    "            \n",
    "            # JSON 응답 파싱\n",
    "            llm_response_content = response.choices[0].message.content\n",
    "            extracted_data = json.loads(llm_response_content)\n",
    "            \n",
    "            if isinstance(extracted_data, list):\n",
    "                for req in extracted_data:\n",
    "                    # 'id'가 없으면 생성 (REQ-XXX 형태)\n",
    "                    if 'id' not in req:\n",
    "                        req['id'] = f\"REQ-{len(final_semantic_chunks) + 1:03d}\"\n",
    "                    # source_pages가 없으면 현재 청크의 페이지 번호 추가\n",
    "                    if 'source_pages' not in req or not req['source_pages']:\n",
    "                        req['source_pages'] = page_numbers_in_chunk if page_numbers_in_chunk else [start_page_in_chunk]\n",
    "                    final_semantic_chunks.append(req)\n",
    "            elif isinstance(extracted_data, dict) and 'requirements' in extracted_data and isinstance(extracted_data['requirements'], list):\n",
    "                # { \"requirements\": [...] } 형태의 응답도 처리\n",
    "                for req in extracted_data['requirements']:\n",
    "                    if 'id' not in req:\n",
    "                        req['id'] = f\"REQ-{len(final_semantic_chunks) + 1:03d}\"\n",
    "                    if 'source_pages' not in req or not req['source_pages']:\n",
    "                        req['source_pages'] = page_numbers_in_chunk if page_numbers_in_chunk else [start_page_in_chunk]\n",
    "                    final_semantic_chunks.append(req)\n",
    "            else:\n",
    "                print(f\"경고: 예상치 못한 JSON 형식 응답. {llm_response_content[:100]}...\")\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON 파싱 오류: {e}. 응답: {llm_response_content[:200]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"LLM API 호출 중 오류 발생: {e}\")\n",
    "            \n",
    "    return final_semantic_chunks\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path = \"./data/8. (제안요청서) 요구사항 분석 자동화_AI ITS 혁신팀.pdf\"\n",
    "    \n",
    "    # 1. 전체 텍스트 추출 (페이지 정보 포함)\n",
    "    print(\"PDF 텍스트 추출 중...\")\n",
    "    full_document_text = extract_text_with_page_info(pdf_file_path)\n",
    "    # print(full_document_text[:2000]) # 추출된 텍스트 확인\n",
    "\n",
    "    # 2. 초기 텍스트 분할\n",
    "    print(\"초기 텍스트 분할 중...\")\n",
    "    initial_chunks = initial_text_split(full_document_text, chunk_size=3000, chunk_overlap=300) \n",
    "    # chunk_size는 LLM의 context window를 고려하여 설정 (gpt-4o는 128K 토큰)\n",
    "    # 한글은 영문보다 토큰이 많이 소요되므로 실제 토큰 수를 고려해야 함\n",
    "    # 3000 글자 = 대략 1000~1500 토큰 예상\n",
    "    print(f\"총 {len(initial_chunks)}개의 초기 청크가 생성되었습니다.\")\n",
    "    \n",
    "    # 3. LLM을 이용한 의미적 청킹 (요구사항 추출)\n",
    "    print(\"\\nLLM을 이용한 의미적 청킹 (요구사항 추출) 시작...\")\n",
    "    extracted_requirements = llm_based_semantic_chunking(initial_chunks, llm_model=\"gpt-4o\") # gpt-4o 또는 gpt-3.5-turbo 선택\n",
    "    \n",
    "    print(f\"\\n총 {len(extracted_requirements)}개의 요구사항이 추출되었습니다.\")\n",
    "    print(\"\\n--- 추출된 요구사항 미리보기 (상위 5개) ---\")\n",
    "    for i, req in enumerate(extracted_requirements[:5]):\n",
    "        print(f\"ID: {req.get('id', 'N/A')}\")\n",
    "        print(f\"Type: {req.get('type', 'N/A')}\")\n",
    "        print(f\"Description: {req.get('description', 'N/A')}\")\n",
    "        print(f\"Source Pages: {req.get('source_pages', 'N/A')}\")\n",
    "        print(f\"Raw Text Snippet: {req.get('raw_text_snippet', 'N/A')[:100]}...\") # 원본 스니펫 일부\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-nqLnFQ7l-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
